<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
 
  
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
  <link rel="icon" href="/assets/favicon.ico">
   <title> Study Guide - mdo-lab </title> 
   <meta name="description" content="Study guide (mdo-2021)" /> 

   <meta property="og:title" content="Study Guide" /> 
   <meta property="og:type" content="article" /> 
   <meta property="og:description" content="Study guide (mdo-2021)" /> 
  

   <meta name="twitter:title" content="Study Guide" /> 
   <meta name="twitter:card" content="summary" /> 
   <meta name="twitter:description" content="Study guide (mdo-2021)" /> 
  
</head>
<body>
  <header>
<div class="blog-name"><a href="/">mdo-lab</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/mdo/">mdo</a></li>
  </ul>
  <img src="/assets/hamburger.svg" id="menu-icon">
</nav>
</header>


<div class="franklin-content">
   <h1>Study Guide</h1> 
</div>
<div class="franklin-content">
<div class="franklin-toc"><ol><li><a href="#gradient_methods">Gradient methods</a><ol><li><a href="#gradient_descent_algorithms">Gradient descent algorithms</a><ol><li><a href="#step-size_calculation">Step-size Calculation</a><ol><li><a href="#exact_step-size_calculation">Exact step-size calculation</a></li><li><a href="#iterative_step-size_calculation">Iterative step-size calculation</a></li><li><a href="#inexact_line_search">Inexact line search</a></li></ol></li><li><a href="#drawbacks_of_steepest_descent_method">Drawbacks of steepest descent method</a></li></ol></li></ol></li><li><a href="#stoppage_criteria">Stoppage criteria</a></li><li><a href="#algorithm_performance">Algorithm Performance</a><ol><li><a href="#computational_complexity_space_and_time">Computational complexity &#40;space and time&#41;</a></li><li><a href="#rate_of_convergence">Rate of convergence</a></li><li><a href="#calculation_of_gradient_and_hessian">Calculation of Gradient &#40;and Hessian&#41;</a><ol><li><a href="#finite_difference">Finite Difference</a></li></ol></li></ol></li></ol></div>
<h2 id="gradient_methods"><a href="#gradient_methods">Gradient methods</a></h2>
<ol>
<li><p>Line search methods</p>
<ul>
<li><p>Gradient descent algorithms</p>
<ul>
<li><p>Steepest descent</p>
</li>
<li><p>Coordinate descent</p>
</li>
<li><p>Stochastic gradient descent</p>
</li>
<li><p>Conjugate gradient</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Trust region methods</p>
</li>
</ol>
<h3 id="gradient_descent_algorithms"><a href="#gradient_descent_algorithms">Gradient descent algorithms</a></h3>
<ul>
<li><p>Select a descent direction</p>
</li>
<li><p>Select a step-size &#40;\(\alpha\)&#41; along the descent direction</p>
</li>
</ul>
<h4 id="step-size_calculation"><a href="#step-size_calculation">Step-size Calculation</a></h4>
<p>Step-size calculation methods can be classified as:</p>
<h5 id="exact_step-size_calculation"><a href="#exact_step-size_calculation">Exact step-size calculation</a></h5>
<span style="font-size:85%;line-height:0em;color:maroon;">&#40; what is it? - why is it useful? - Can I do it for engineering problems? - calculation of exact step-size for a symmetric positive definite quadratic objective function - show \(\bar{c}^{(k+1)^T} \bar{d}^{(k)} = 0\) for exact step-size calculation &#41;</span>
<h5 id="iterative_step-size_calculation"><a href="#iterative_step-size_calculation">Iterative step-size calculation</a></h5>
<span style="font-size:85%;line-height:0em;color:maroon;">&#40; why required - how do we compare algorithm performance - different algorithms &#41;</span>
<p>Step-size calculation usually happens in two steps:</p>
<ol>
<li><p>Bracketing the minima</p>
<ul>
<li><p>Equal interval search</p>
</li>
</ul>
</li>
<li><p>Reduction in the interval of uncertainty &#40;or the bracket&#41; iteratively</p>
<ul>
<li><p>Equal interval search &#40;1-point and 2-point methods&#41;</p>
</li>
<li><p>Golden-section search</p>
</li>
<li><p>Polynomial interpolation</p>
<ul>
<li><p>Quadratic curve fitting</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h5 id="inexact_line_search"><a href="#inexact_line_search">Inexact line search</a></h5>
<ul>
<li><p>Armijo&#39;s rule</p>
</li>
</ul>
<h4 id="drawbacks_of_steepest_descent_method"><a href="#drawbacks_of_steepest_descent_method">Drawbacks of steepest descent method</a></h4>
<h2 id="stoppage_criteria"><a href="#stoppage_criteria">Stoppage criteria</a></h2>
<span style="font-size:85%;line-height:0em;color:maroon;">&#40; Assignment 1 &#41;</span>
<ul>
<li><p>Similar criteria is used for gradient as well as non-gradient methods</p>
</li>
</ul>
<h2 id="algorithm_performance"><a href="#algorithm_performance">Algorithm Performance</a></h2>
<h3 id="computational_complexity_space_and_time"><a href="#computational_complexity_space_and_time">Computational complexity &#40;space and time&#41;</a></h3>
<h3 id="rate_of_convergence"><a href="#rate_of_convergence">Rate of convergence</a></h3>
<span style="font-size:85%;line-height:0em;color:maroon;">&#40; what is it? - why is it required? - can I calculate it in engineering problems? -linear convergence - superlinear convergence - quadratic convergence &#41;</span>
\[\beta = \frac{\left\lVert  \bar{x}^{(k+1)} - \bar{x}^* \right\rVert_{2} }{\left\lVert  \bar{x}^{(k)} - \bar{x}^* \right\rVert_{2}^r}\]
<p>where \(\beta\) is convergence ratio and \(r\) is the rate of convergence.</p>
<table><tr><th align="right">Convergence</th><th align="right">Condition</th></tr><tr><td align="right">Linear</td><td align="right">\(r=1\) and \(\beta < 1\) &#40;Only first order information required&#41;</td></tr><tr><td align="right">Superlinear</td><td align="right">\(r=1\) and \(\beta \rightarrow 0\)</td></tr><tr><td align="right">Superlinear</td><td align="right">\(1 < r<2\) and \(\beta < 1\) &#40;Achieved using gradient information from previous steps&#41;</td></tr><tr><td align="right">Quadratic</td><td align="right">\(r=2\) and \(\beta<1\) &#40;Hessian information required&#41;</td></tr></table>
<div class="blank"></div><span style="font-size:85%;line-height:0em;color:DarkSlateGray;"><strong>Q</strong>:  Will an algorithm with quadratic convergence always converge faster than a linear algorithm? If the answer is not affirmative, give an example.</span>
<h3 id="calculation_of_gradient_and_hessian"><a href="#calculation_of_gradient_and_hessian">Calculation of Gradient &#40;and Hessian&#41;</a></h3>
<ul>
<li><p>Finite Difference</p>
</li>
<li><p>Complex variable trick</p>
</li>
<li><p>Automatic Differentiation</p>
</li>
<li><p>Hyperdual numbers</p>
</li>
</ul>
<h4 id="finite_difference"><a href="#finite_difference">Finite Difference</a></h4>
<span style="font-size:85%;line-height:0em;color:maroon;">&#40; Assignment 3 &#41;</span>
<div class="blank"></div><span style="font-size:85%;line-height:0em;color:DarkSlateGray;"><strong>Q</strong>:  What is the time complexity of gradient and Hessian calculation for forward and central finite difference method?</span>
<div class="blank"></div><span style="font-size:85%;line-height:0em;color:DarkSlateGray;"><strong>Q</strong>:  Can we apply finite difference method for implicit objective functions?</span>
<div class="blank"></div><span style="font-size:85%;line-height:0em;color:DarkSlateGray;"><strong>Q</strong>:  What are the advantages and disadvantages of finite difference gradient calculation?</span>
<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Devendra Ghate. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
